# hercules_medusa_watcher

# ğŸ•µï¸â€â™€ï¸ hercules_medusa_watcher â€” Video Analysis Agent

[**Hercules**](https://github.com/test-zeus-ai/testzeus-hercules) is an autonomous agent that executes software test cases by interacting with user interfaces in virtual environments.

This repository contains the **Medusa Watcher**, a companion that verifies whether a Hercules test run was executed as planned.  
It compares the agentâ€™s internal planning logs, the actual video recording of the run, and the final test result to determine if the execution aligned with the intended plan.

---

## ğŸ§  What This Repository Does

This pipeline evaluates test runs by:

- ğŸ” Parsing the **planner logs** generated by Hercules (`agent_inner_logs.json`)
- ğŸ¥ Extracting key frames from the **execution video** (`video.webm`)
- ğŸ§¾ Using a **multimodal LLM** to perform OCR + captioning on UI frames
- âœ… Matching the planned steps to visual evidence in frames
- ğŸ“Š Comparing this with the **final HTML test report**
- ğŸ§  Calling a final LLM to determine alignment, missed steps, and inconsistencies

---

## ğŸ§  Key Features

* **Multimodal Video Analysis** with Qwen2VL-7B using ColPali (used byaldi wrapper).
* **Planning vs Execution Matching** using LLMs.
* **Deviation Detection** using final test output.
* **Fully Modular Pipeline** â€” each stage is separable & testable.

---

## ğŸ“ Inputs & Structure

Place the following in the `/data/shared/users/antara/rag/video/` folder:

```bash
/data/shared/users/antara/rag/video/
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ agent_inner_logs.json        # Planning logs (JSON)
â”‚   â””â”€â”€ test_report.html             # Final test report (HTML)
â”œâ”€â”€ media/
â”‚   â””â”€â”€ test_video.webm              # Video of the test execution
â””â”€â”€ output/                          # All intermediate + final outputs
```

---

## ğŸš€ How to Run (Manual Flow)

```bash
# 1. Extract structured steps from logs
python parser.py

# 2. Extract frames from video
python frames.py

# 3. OCR + Captioning with Qwen2-VL
python ocr.py

# 4. Match plan steps with video evidence (LLM)
python detective.py

# 5. Post-process verification results
python output_postprocess.py

# 6. Deviation analysis via Azure GPT-4o
python azure_gpt.py
```

---

## ğŸ“¦ Output Files

```bash
/output/
â”œâ”€â”€ summary.json                           # Extracted plan steps
â”œâ”€â”€ ocr_caption_results.json               # OCR + captions per frame
â”œâ”€â”€ comparison/
â”‚   â”œâ”€â”€ step_verification_llm.json         # LLM match per step
â”‚   â””â”€â”€ llm_verification_report.json       # Matched vs Missing steps
â””â”€â”€ deviation_report/
    â””â”€â”€ final_alignment_report.json        # âœ… Final deviation report
```

---

## ğŸ§± Architecture

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Agent Logs â”‚     â”‚ Video.mp4  â”‚     â”‚ HTML Reportâ”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“                 â†“                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Plan Extract â”‚   â”‚ Frame Captureâ”‚   â”‚ Report Parse â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“                  â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Qwen2VL: OCR + Captioning       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Step Verification (LLM match)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Azure GPT-4o Final Comparison     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Module Overview

| File                    | Purpose                                                 |
| ----------------------- | ------------------------------------------------------- |
| `parser.py`             | Parses inner agent logs into structured plan steps.     |
| `frames.py`             | Converts test video into per-second frames.             |
| `ocr.py`                | Uses Qwen2-VL to perform OCR + captioning.              |
| `detective.py`          | Compares steps to frames using LLM to verify alignment. |
| `output_postprocess.py` | Summarizes matched vs missing steps.                    |
| `azure_gpt.py`          | Uses GPT-4o to detect final execution deviations.       |

---

## ğŸ“Œ Final Output Format

`final_alignment_report.json`

```json
{
  "steps_aligned": ["Step_1", "Step_2", ...],
  "steps_with_deviation": [
    {
      "step_id": 3,
      "step_description": "Click 'Login' button",
      "video_evidence": "Missing",
      "test_output_result": "Login failed",
      "reason": "Step not observed in video, but test expected success."
    }
  ],
  "overall_alignment_status": "partially_aligned",
  "final_result": "âš ï¸ Agent deviated from expected behavior."
}
```

---

## ğŸ› ï¸ Dependencies

```bash
pip install -r requirements.txt
```

Key packages:

* `transformers`, `torch`
* `byaldi`, `qwen2vl`, `Pillow`
* `bs4`, `openai` (Azure)
* `opencv-python`, `tqdm`

---

## ğŸ”® Future Plans

- Full **Agentic Workflow**: Auto-decide next steps based on intermediate results.
- **UI Integration**: Visual interface to upload files and view deviations.
- **Command Line Package**: Unified CLI with progress bars and config-based execution.
- **Docker**

---

---

## ğŸ§ª Experiments & Research

### ğŸ” Previous Experiments

1. **Agentic Workflow (Reflection + Tool Use)**  
   - Built a planning agent using reflection
   - video watcher agent using tool use + reflection
   - However, output quality degraded with multi-agent hops â€” paused this in favor of a stable core logic.
   - Codebase retained under the `experiments/` folder.

2. **PixelTable Integration**  
   - Explored using PixelTable for interactive multimodal logs and frame inspection.
   - Found it powerful but too heavy for rapid iteration under deadline pressure.
   - Logged under `experiments/`.

3. **Testing different model and evaluation**
   - Started with OCR pipelines and traditional models, found Qwen 2 VL 7B strong for OCR tasks and eval.
   - For rest task experimented with light weight LLMs, found GPT 4o best but needs much post processing it was best for logging and final report.
   - Qwen 2 for general tasks.

### ğŸ“š References & Future Reading

- **Agentic Reasoning**  
  [Multimodal AI: Cracking the Code](https://multimodalai.substack.com/p/cracking-the-code-of-multimodal-ai)  
  [How Agents Improve LLM Performance (DeepLearning.ai)](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io)

- **Papers to Explore**  
  - [VideoRAG: Retrieval-Augmented Generation over Video Corpus (2501.05874)](https://arxiv.org/abs/2501.05874)  
  - [dvancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs (2406.07476)](https://arxiv.org/abs/2406.07476)  
  - [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection (2311.10122)](https://arxiv.org/abs/2311.10122)

---

## ğŸ™Œ Author

Antara Raman Sahay | July 2025

---

