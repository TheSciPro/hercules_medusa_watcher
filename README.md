# hercules_medusa_watcher

# 🕵️‍♀️ hercules_medusa_watcher — Video Analysis Agent

[**Hercules**](https://github.com/test-zeus-ai/testzeus-hercules) is an autonomous agent that executes software test cases by interacting with user interfaces in virtual environments.

This repository contains the **Medusa Watcher**, a companion that verifies whether a Hercules test run was executed as planned.  
It compares the agent’s internal planning logs, the actual video recording of the run, and the final test result to determine if the execution aligned with the intended plan.

---

## 🧠 What This Repository Does

This pipeline evaluates test runs by:

- 🔍 Parsing the **planner logs** generated by Hercules (`agent_inner_logs.json`)
- 🎥 Extracting key frames from the **execution video** (`video.webm`)
- 🧾 Using a **multimodal LLM** to perform OCR + captioning on UI frames
- ✅ Matching the planned steps to visual evidence in frames
- 📊 Comparing this with the **final HTML test report**
- 🧠 Calling a final LLM to determine alignment, missed steps, and inconsistencies

---

## 🧠 Key Features

* **Multimodal Video Analysis** with Qwen2VL-7B using ColPali (used byaldi wrapper).
* **Planning vs Execution Matching** using LLMs.
* **Deviation Detection** using final test output.
* **Fully Modular Pipeline** — each stage is separable & testable.

---

## 📁 Inputs & Structure

Place the following in the `/data/shared/users/antara/rag/video/` folder:

```bash
/data/shared/users/antara/rag/video/
├── logs/
│   ├── agent_inner_logs.json        # Planning logs (JSON)
│   └── test_report.html             # Final test report (HTML)
├── media/
│   └── test_video.webm              # Video of the test execution
└── output/                          # All intermediate + final outputs
```

---

## 🚀 How to Run (Manual Flow)

```bash
# 1. Extract structured steps from logs
python parser.py

# 2. Extract frames from video
python frames.py

# 3. OCR + Captioning with Qwen2-VL
python ocr.py

# 4. Match plan steps with video evidence (LLM)
python detective.py

# 5. Post-process verification results
python output_postprocess.py

# 6. Deviation analysis via Azure GPT-4o
python azure_gpt.py
```

---

## 📦 Output Files

```bash
/output/
├── summary.json                           # Extracted plan steps
├── ocr_caption_results.json               # OCR + captions per frame
├── comparison/
│   ├── step_verification_llm.json         # LLM match per step
│   └── llm_verification_report.json       # Matched vs Missing steps
└── deviation_report/
    └── final_alignment_report.json        # ✅ Final deviation report
```

---

## 🧱 Architecture

```
        ┌────────────┐     ┌────────────┐     ┌────────────┐
        │ Agent Logs │     │ Video.mp4  │     │ HTML Report│
        └────┬───────┘     └────┬───────┘     └────┬───────┘
             ↓                 ↓                 ↓
     ┌──────────────┐   ┌──────────────┐   ┌──────────────┐
     │ Plan Extract │   │ Frame Capture│   │ Report Parse │
     └────┬─────────┘   └────┬─────────┘   └──────────────┘
          ↓                  ↓
  ┌───────────────────────────────────┐
  │   Qwen2VL: OCR + Captioning       │
  └───────────────────────────────────┘
                      ↓
  ┌───────────────────────────────────┐
  │  Step Verification (LLM match)    │
  └───────────────────────────────────┘
                      ↓
  ┌───────────────────────────────────┐
  │ Azure GPT-4o Final Comparison     │
  └───────────────────────────────────┘
```

---

## 🧩 Module Overview

| File                    | Purpose                                                 |
| ----------------------- | ------------------------------------------------------- |
| `parser.py`             | Parses inner agent logs into structured plan steps.     |
| `frames.py`             | Converts test video into per-second frames.             |
| `ocr.py`                | Uses Qwen2-VL to perform OCR + captioning.              |
| `detective.py`          | Compares steps to frames using LLM to verify alignment. |
| `output_postprocess.py` | Summarizes matched vs missing steps.                    |
| `azure_gpt.py`          | Uses GPT-4o to detect final execution deviations.       |

---

## 📌 Final Output Format

`final_alignment_report.json`

```json
{
  "steps_aligned": ["Step_1", "Step_2", ...],
  "steps_with_deviation": [
    {
      "step_id": 3,
      "step_description": "Click 'Login' button",
      "video_evidence": "Missing",
      "test_output_result": "Login failed",
      "reason": "Step not observed in video, but test expected success."
    }
  ],
  "overall_alignment_status": "partially_aligned",
  "final_result": "⚠️ Agent deviated from expected behavior."
}
```

---

## 🛠️ Dependencies

```bash
pip install -r requirements.txt
```

Key packages:

* `transformers`, `torch`
* `byaldi`, `qwen2vl`, `Pillow`
* `bs4`, `openai` (Azure)
* `opencv-python`, `tqdm`

---

## 🔮 Future Plans

- Full **Agentic Workflow**: Auto-decide next steps based on intermediate results.
- **UI Integration**: Visual interface to upload files and view deviations.
- **Command Line Package**: Unified CLI with progress bars and config-based execution.
- **Docker**

---

---

## 🧪 Experiments & Research

### 🔍 Previous Experiments

1. **Agentic Workflow (Reflection + Tool Use)**  
   - Built a planning agent using reflection
   - video watcher agent using tool use + reflection
   - However, output quality degraded with multi-agent hops — paused this in favor of a stable core logic.
   - Codebase retained under the `experiments/` folder.

2. **PixelTable Integration**  
   - Explored using PixelTable for interactive multimodal logs and frame inspection.
   - Found it powerful but too heavy for rapid iteration under deadline pressure.
   - Logged under `experiments/`.

3. **Testing different model and evaluation**
   - Started with OCR pipelines and traditional models, found Qwen 2 VL 7B strong for OCR tasks and eval.
   - For rest task experimented with light weight LLMs, found GPT 4o best but needs much post processing it was best for logging and final report.
   - Qwen 2 for general tasks.

### 📚 References & Future Reading

- **Agentic Reasoning**  
  [Multimodal AI: Cracking the Code](https://multimodalai.substack.com/p/cracking-the-code-of-multimodal-ai)  
  [How Agents Improve LLM Performance (DeepLearning.ai)](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io)

- **Papers to Explore**  
  - [VideoRAG: Retrieval-Augmented Generation over Video Corpus (2501.05874)](https://arxiv.org/abs/2501.05874)  
  - [dvancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs (2406.07476)](https://arxiv.org/abs/2406.07476)  
  - [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection (2311.10122)](https://arxiv.org/abs/2311.10122)

---

## 🙌 Author

Antara Raman Sahay | July 2025

---

